<!-- import aframe and then ar.js with image tracking / location based features -->
<script src="https://cdn.jsdelivr.net/gh/aframevr/aframe@1c2407b26c61958baa93967b5412487cd94b290b/dist/aframe-master.min.js"></script>
<script src="https://raw.githack.com/AR-js-org/AR.js/master/aframe/build/aframe-ar-nft.js"></script>

<!-- style for the loader -->
<style>
  .arjs-loader {
    height: 100%;
    width: 100%;
    position: absolute;
    top: 0;
    left: 0;
    background-color: rgba(0, 0, 0, 0.8);
    z-index: 9999;
    display: flex;
    justify-content: center;
    align-items: center;
  }

  .arjs-loader div {
    text-align: center;
    font-size: 1.25em;
    color: white;
  }
</style>

<body style="margin : 0px; overflow: hidden;">
  <!-- minimal loader shown until image descriptors are loaded. Loading may take a while according to the device computational power -->
  <div class="arjs-loader">
    <div>Loading, please wait...</div>
  </div>

  <!-- a-frame scene -->
  <a-scene
    vr-mode-ui="enabled: false;"
    renderer="logarithmicDepthBuffer: true;"
    embedded
    arjs="trackingMethod: best; sourceType: webcam;debugUIEnabled: false;"
  >
    <!-- a-nft is the anchor that defines an Image Tracking entity -->
    <!-- on 'url' use the path to the Image Descriptors created before. -->
    <!-- the path should end with the name without the extension e.g. if file is 'pinball.fset' the path should end with 'pinball' -->
    <a-nft
      type="nft"
      url="<path-to-your-image-descriptors>"
      smooth="true"
      smoothCount="10"
      smoothTolerance=".01"
      smoothThreshold="5"
    >
      <!-- as a child of the a-nft entity, you can define the content to show. here's a GLTF model entity -->
      <a-entity
        gltf-model="<path-to-your-model>"
        scale="5 5 5"
        position="50 150 0"
      >
      </a-entity>
    </a-nft>
    <!-- static camera that moves according to the device movemenents -->
    <a-entity camera></a-entity>
  </a-scene>
</body>
    renderer="logarithmicDepthBuffer:true"
    embedded arjs="trackingMethod: best; sourceType: webcam; debugUIEnabled: false;"
    >
     <!--VR UI模組不顯示-->
     <!--WEB渲染器 增加AR模型環境遠近距離的渲染精度 顯示時更加穩定--> 
     <!--嵌入arjs 自動挑選最佳追蹤法 用鏡頭作為輸出來源 不顯示除錯畫面--> 



     <!--載入影片資源-->    
        <a-assets>
            <video id="ARVideo" autoplay loop muted playsinline src="./assets/try_2.mp4"></video>
        </a-assets>
        <!--autoplay自動播放 loop循環播放 muted靜音 playsinline影片在網頁內坎播放(不跳全螢幕)--> 

 
    
        <a-nft
        type="nft"
        url="./assets/mathbook_try"
        smooth="true"
        smoothCount="10"
        smoothTolerance="0.01"
        smoothThreshold="5"
        >
         <!--取最近 10 幀的平均位置來決定模型位置,當模型的移動小於0.01時，會被忽略（不更新位置）,當模型移動超過5時，才會重設平滑-->
            <a-video
            src="#ARVideo"
            width="1"
            height="0.5625"
            position="0 0 0"
            rotation="-90 0 0"
            ></a-video>
        </a-nft>



        <!--讓A-Frame 用裝置鏡頭來「看世界」--> 
        <a-entity camera></a-entity>
    </a-scene>
</body>
</html>
